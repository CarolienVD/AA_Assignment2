{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option(\"display.max_colwidth\", None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_pickle(\"intermediate/df_train.pkl\") "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3",
   "metadata": {},
   "source": [
    "# Delete noisy images from training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to remove black / dark borders from image\n",
    "# threshold = pixel intensity threshold to consider a pixel as non-black\n",
    "\n",
    "def auto_crop_black_borders(img, threshold=10):\n",
    "    \n",
    "    if len(img.shape) == 3:\n",
    "        gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    else:\n",
    "        gray = img\n",
    "\n",
    "    # Create a binary mask of non-black pixels\n",
    "    mask = gray > threshold\n",
    "\n",
    "    # Find the bounding box of the non-black area\n",
    "    rows = np.any(mask, axis=1)\n",
    "    cols = np.any(mask, axis=0)\n",
    "\n",
    "    if not np.any(rows) or not np.any(cols):\n",
    "        return img  # nothing to crop\n",
    "\n",
    "    y_min, y_max = np.where(rows)[0][[0, -1]]\n",
    "    x_min, x_max = np.where(cols)[0][[0, -1]]\n",
    "\n",
    "    cropped = img[y_min:y_max+1, x_min:x_max+1]\n",
    "    return cropped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to identify images dominated by one color -> remove bad DQ cases\n",
    "\n",
    "def split_by_color_dominance(df, path_col, threshold=0.25):\n",
    "    good, bad = [], []\n",
    "\n",
    "    for _, row in tqdm(df.iterrows(), total=len(df)):\n",
    "        path = row[path_col]\n",
    "        img = cv2.imread(path)\n",
    "        if img is None:\n",
    "            continue\n",
    "\n",
    "        img = auto_crop_black_borders(img)\n",
    "        b, g, r = cv2.split(img)\n",
    "        total = b.astype(np.float32) + g + r + 1e-5\n",
    "        ratios = [np.mean(c / total) for c in (r, g, b)]\n",
    "\n",
    "        (good if min(ratios) > threshold else bad).append(row)\n",
    "\n",
    "    return pd.DataFrame(good), pd.DataFrame(bad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split up train data into clean and unclean images\n",
    "\n",
    "df_train_clean, df_train_unclean = split_by_color_dominance(df_train, 'full_path')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_clean.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_unclean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "img = cv2.imread(\"images/italy/1741644879_46.4328734_13.6278.jpg\")\n",
    "img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)  # Convert from BGR to RGB\n",
    "img = auto_crop_black_borders(img)\n",
    "\n",
    "plt.imshow(img)\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_clean = df_train_clean.reset_index()\n",
    "df_train_clean.tail"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_clean.to_pickle(\"intermediate/df_train_clean.pkl\") \n",
    "df_train_unclean.to_pickle(\"intermediate/df_train_unclean.pkl\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_clean = pd.read_pickle(\"intermediate/df_train_clean.pkl\") "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13",
   "metadata": {},
   "source": [
    "# Slice training images into square images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to convert equirectangular ('panoramic') image to perspective ('normal') image\n",
    "\n",
    "# fov = horizontal field of view (degrees)\n",
    "# theta = rotation angle (degrees)\n",
    "# size = output image size\n",
    "\n",
    "def equirectangular_to_perspective(equi_img, fov, theta, size=512):\n",
    "\n",
    "    height, width = size, size\n",
    "    equ_h, equ_w = equi_img.shape[:2]\n",
    "\n",
    "    # Convert angles to radians\n",
    "    fov_rad = np.deg2rad(fov)\n",
    "    theta_rad = np.deg2rad(theta)\n",
    "\n",
    "    # Grid of x, y in normalized view space\n",
    "    x = np.linspace(-np.tan(fov_rad / 2), np.tan(fov_rad / 2), width)\n",
    "    y = np.linspace(-1, 1, height)  # keep vertical stretch simple\n",
    "    x, y = np.meshgrid(x, -y)  # flip y for image orientation\n",
    "    z = np.ones_like(x)\n",
    "\n",
    "    # Normalize direction vectors\n",
    "    norm = np.sqrt(x**2 + y**2 + z**2)\n",
    "    x /= norm\n",
    "    y /= norm\n",
    "    z /= norm\n",
    "\n",
    "    # Rotate around Y axis (theta)\n",
    "    x_rot = np.cos(theta_rad) * x + np.sin(theta_rad) * z\n",
    "    z_rot = -np.sin(theta_rad) * x + np.cos(theta_rad) * z\n",
    "\n",
    "    # Convert to spherical coordinates\n",
    "    lon = np.arctan2(x_rot, z_rot)\n",
    "    lat = np.arcsin(y)\n",
    "\n",
    "    # Map to image coordinates\n",
    "    u = (lon + np.pi) / (2 * np.pi) * equ_w\n",
    "    v = (np.pi / 2 - lat) / np.pi * equ_h\n",
    "\n",
    "    # Remap\n",
    "    u = u.astype(np.float32)\n",
    "    v = v.astype(np.float32)\n",
    "    perspective = cv2.remap(equi_img, u, v, interpolation=cv2.INTER_LINEAR, borderMode=cv2.BORDER_WRAP)\n",
    "\n",
    "    return perspective"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function that first removes black borders, then converts it to 'normal' perspective, then slices it into 4 images and saves them\n",
    "\n",
    "def slice_square_images(image_path, size = 512):\n",
    "    img = cv2.imread(image_path)\n",
    "    img = auto_crop_black_borders(img)  # Remove black borders if needed\n",
    "\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB) \n",
    "\n",
    "    views = ['front', 'right', 'back', 'left']\n",
    "\n",
    "    image_name = image_path.split('/')[2].replace('.jpg', '')\n",
    "    country = image_path.split('/')[1]\n",
    "    \n",
    "    output_folder = 'train_clean_images_square/' + country + '/'\n",
    "    if not os.path.exists(output_folder):\n",
    "        os.makedirs(output_folder)\n",
    "\n",
    "    for i in range(4):  \n",
    "        view = equirectangular_to_perspective(img, fov=90, theta=i*90, size = size)\n",
    "\n",
    "        plt.imshow(view)\n",
    "        plt.axis('off')\n",
    "        plt.show()\n",
    "        \n",
    "        new_image_path = os.path.join(output_folder + f\"{image_name}_{views[i]}.jpg\")\n",
    "        cv2.imwrite(new_image_path, view)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "slice_square_images('images/japan/1741695546_36.823432_139.5921591.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# slice all training images into 4 square images\n",
    "\n",
    "tqdm.pandas() # to see a progress bar\n",
    "\n",
    "df_train_clean['full_path'].progress_apply(slice_square_images)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18",
   "metadata": {},
   "source": [
    "# Make new df with paths of all square images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_paths_of_square_images(path):\n",
    "    new_paths = []\n",
    "    views = ['front', 'right', 'back', 'left']\n",
    "    parts = path.replace(\".jpg\", \"\").split(\"/\")\n",
    "    for view in views:\n",
    "        path = 'train_clean_images_square/' + f\"{parts[1]}/{parts[2]}_{view}.jpg\"\n",
    "        new_paths.append(path)\n",
    "    return new_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply the function to 'path' column and create a new column 'new_paths'\n",
    "\n",
    "df_train_clean['sq_image_path'] = df_train_clean['full_path'].apply(get_paths_of_square_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# explode the new column into multiple rows\n",
    "\n",
    "df_train_clean_sq = df_train_clean.explode('sq_image_path', ignore_index=True).copy()\n",
    "\n",
    "df_train_clean_sq.drop(['level_0', 'filename', 'color', 'set'], axis = 1, inplace = True) # remove irrelevant columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert region cluster to str variable to use as classification target variable\n",
    "\n",
    "df_train_clean_sq['region_cluster_str'] = \"region_\" + df_train_clean_sq['region_cluster'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_clean_sq.tail(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_clean_sq.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25",
   "metadata": {},
   "source": [
    "# Finetune model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.applications import EfficientNetB0\n",
    "from tensorflow.keras import layers, models\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.models import Model, Sequential\n",
    "from tensorflow.keras.layers import Dense, GlobalAveragePooling2D, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make train and validation set, making sure that slices from the same original image are kept together\n",
    "\n",
    "from sklearn.model_selection import GroupShuffleSplit\n",
    "\n",
    "splitter = GroupShuffleSplit(n_splits=1, test_size=0.25, random_state=16)\n",
    "train_idx, val_idx = next(splitter.split(df_train_clean_sq, groups=df_train_clean_sq['full_path']))\n",
    "\n",
    "train_df_gen = df_train_clean_sq.iloc[train_idx]\n",
    "val_df_gen = df_train_clean_sq.iloc[val_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create separate train and validation set + normalize pixel values\n",
    "\n",
    "train_val_datagen = ImageDataGenerator(rescale=1/255) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_generator = train_val_datagen.flow_from_dataframe(\n",
    "    dataframe= train_df_gen,  \n",
    "    x_col='sq_image_path',\n",
    "    y_col='country',\n",
    "    #y_col = 'region_cluster_str',\n",
    "    target_size=(224, 224),  # resize images to fit the model\n",
    "    batch_size=16,\n",
    "    class_mode='categorical',\n",
    "    shuffle=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# note: slices from the same picture will be spread across train and val!\n",
    "\n",
    "validation_generator = train_val_datagen.flow_from_dataframe(\n",
    "    dataframe= val_df_gen,  \n",
    "    x_col='sq_image_path',\n",
    "    y_col='country',\n",
    "    #y_col = 'region_cluster_str',\n",
    "    target_size=(224, 224),  # resize images to fit the model\n",
    "    batch_size=16,\n",
    "    class_mode='categorical',\n",
    "    shuffle=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load EfficientNetB0 without the top layer (classification layer)\n",
    "\n",
    "base_model = EfficientNetB0(weights='imagenet', include_top=False, input_shape=(224, 224, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# freeze the base model layers\n",
    "\n",
    "base_model.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create sequential model\n",
    "\n",
    "model_finetune = Sequential()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define data augmentations that are relevant (realistic) for these images\n",
    "\n",
    "data_augmentation = Sequential([\n",
    "    layers.RandomFlip('horizontal'),   # Flip images horizontally\n",
    "    layers.RandomBrightness(0.2),      # Randomly adjust the brightness by up to 20%\n",
    "    layers.RandomContrast(0.1),\n",
    "    layers.RandomZoom(0.2)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add the data augmentation layers\n",
    "\n",
    "model_finetune.add(data_augmentation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add the frozen base model\n",
    "\n",
    "model_finetune.add(base_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add a global average pooling layer to reduce dimensionality\n",
    "\n",
    "model_finetune.add(GlobalAveragePooling2D())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add dense layers, include dropout layers to avoid overfitting\n",
    "\n",
    "model_finetune.add(Dense(256, activation='relu'))\n",
    "model_finetune.add(Dropout(0.4))\n",
    "model_finetune.add(Dense(64, activation='relu'))\n",
    "model_finetune.add(Dropout(0.3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add a final dense layer used for prediction\n",
    "\n",
    "model_finetune.add(Dense(df_train_clean_sq['country'].nunique(), activation='softmax')) \n",
    "#model_finetune.add(Dense(df_train_clean_sq['region_cluster_str'].nunique(), activation='softmax')) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_finetune.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(repr(model_finetune))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_finetune.compile(optimizer=Adam(learning_rate=0.001), loss='categorical_crossentropy', metrics=['accuracy']) # set learning rate less small?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43",
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model_finetune.fit(\n",
    "    train_generator,\n",
    "    epochs=20,\n",
    "    validation_data=validation_generator,\n",
    "    callbacks=[\n",
    "        EarlyStopping(patience=3, restore_best_weights=True),  # Stops early if validation performance doesn't improve\n",
    "        ModelCheckpoint('best_model_2.keras', save_best_only=True)  # Save the best model during training\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fine-tune the model: unfreeze some layers\n",
    "\n",
    "base_model.trainable = True\n",
    "for layer in base_model.layers[:100]:\n",
    "    layer.trainable = False  # freeze the first 100 layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_finetune.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_finetune.compile(optimizer=Adam(learning_rate=0.00001), loss='categorical_crossentropy', metrics=['accuracy']) # set learning rate less small?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47",
   "metadata": {},
   "outputs": [],
   "source": [
    "early_stopping = EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    patience=3,\n",
    "    restore_best_weights=True,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "checkpoint = ModelCheckpoint(\n",
    "    'best_model_6.keras',\n",
    "    monitor='val_loss',\n",
    "    save_best_only=True,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "history = model_finetune.fit(\n",
    "    train_generator,\n",
    "    epochs=20,\n",
    "    validation_data=validation_generator,\n",
    "    callbacks=[early_stopping, checkpoint],\n",
    "    verbose=1  # Optional: shows epoch progress\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_df_gen['region_cluster_str'].value_counts(normalize=True))\n",
    "\n",
    "print(val_df_gen['region_cluster_str'].value_counts(normalize=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_df_gen['country'].value_counts(normalize=True))\n",
    "\n",
    "print(val_df_gen['country'].value_counts(normalize=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
